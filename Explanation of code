Following is a full, detailed explanation of Iris.py:

First, the Iris dataset is loaded from the Scikit-learn library using the load_iris() function. The feature data and target data are stored in the X and y variables, respectively.

Next, the data is split into training and testing sets using the train_test_split() function from the model_selection module. 
The training set is used to train the models, while the testing set is used to evaluate their performance.
Next, the data is scaled to prepare the feature data for modeling. I have chosen to scale the data using the StandardScaler() function from the preprocessing module as it yields the highest accuracies for each of the three classifiers. The data unscaled provides accuracies as .899, .8, and .977 for linear regression, Perceptron, and logistic regression respectively. Scaling the data with the MinMax scaler yields accuracies as .899, .77, and .88 for linear regression , Perceptron, and logistic regression respectively. Finally using the standard scaler, accuracies for each of the models were .89, .95, and .977 for linear regression, Perceptron, and logistic regression respectively.
After scaling the data, the three linear models are trained and evaluated.
The first model trained is the Linear regression classification model which is trained using the LinearRegression() function from the linear_model module. The fit() method is implemented to train the model on the rescaled training data. The predict() method is used to generate predictions on the rescaled testing data. The score() method is then used to calculate the model's performance on the testing set. The model's weights are printed using the coef_ attribute. The analytical generated weights are calculated through linear algebra by using the analytical solution for linear regression, which is calculated using the formula w = (X^T.X)^(-1).X^T.y, where X is the rescaled training feature data, y is the training target data, and w is the analytical solution for the weights. I did this by using “from numpy.linalg import inv” and the .dot operator. This calculates the analytical weights for linear regression by first finding the inverse of the dot product of the transpose, then multiplying this inverse with the transpose of the feature data matrix, and finally multiplying the resulting matrix with the target variable vector. The result is stored in the variable w.
The second model is Perceptron, which is imported and trained using the sklearns built in Perceptron() function from the linear_model module. The max_iter parameter is set to 1000, and the tol parameter is set to 1e-3 to ensure that the model performs enough iterations to train sufficiently. The fit(), Predict(), and Score() functions are used again as explained above. The weights are printed using the .coef_ attribute, and the confusion matrix is printed using the confusion_matrix() function from the metrics module.
   
